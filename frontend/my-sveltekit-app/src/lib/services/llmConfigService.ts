// LLM Configuration Service - Template Independent Agent LLM Management
import {
  type LLMProvider,
  type LLMModel,
  type AgentLLMConfig,
  DEFAULT_LLM_PROVIDERS,
  DEFAULT_LLM_MODELS,
  DEFAULT_AGENT_LLM_CONFIGS,
  getProviderIcon
} from '$lib/types/llm-config';
import dynamicModelsService, {
  type ModelInfo,
  type ProviderInfo
} from '$lib/services/dynamicModelsService';

class LLMConfigurationService {
  private baseUrl = '/api';

  constructor() {
    console.log('ü§ñ LLM CONFIG: Service initialized');
  }

  /**
   * Get all available LLM providers
   */
  async getProviders(): Promise<LLMProvider[]> {
    try {
      console.log('üìã LLM CONFIG: Fetching LLM providers (dynamic)');

      // Prefer dynamic, API-key-aware providers from backend
      const dynamicProviders: ProviderInfo[] = await dynamicModelsService.getProviders();

      if (dynamicProviders && dynamicProviders.length > 0) {
        const mappedProviders: LLMProvider[] = dynamicProviders.map((provider) => {
          // Try to enrich from defaults if we have a matching entry
          const defaultDef = DEFAULT_LLM_PROVIDERS.find((p: LLMProvider) => p.id === provider.id);

          return {
            id: provider.id,
            name: provider.name,
            description: provider.description,
            icon: defaultDef?.icon || getProviderIcon(provider.id),
            // Cast is safe because backend only returns known providers today
            type: (provider.id as any) ?? 'custom',
            baseUrl: defaultDef?.baseUrl,
            apiKeyRequired: true,
            supportedFeatures: defaultDef?.supportedFeatures || ['chat', 'completion']
          };
        });

        console.log(`‚úÖ LLM CONFIG: Loaded ${mappedProviders.length} providers from dynamicModelsService`);
        return mappedProviders;
      }

      // Fallback: static defaults (no API key awareness)
      console.warn('‚ö†Ô∏è LLM CONFIG: dynamicModelsService returned no providers, falling back to DEFAULT_LLM_PROVIDERS');
      return DEFAULT_LLM_PROVIDERS;
      
    } catch (error) {
      console.error('‚ùå LLM CONFIG: Failed to load providers:', error);
      // Fallback to default providers
      return DEFAULT_LLM_PROVIDERS;
    }
  }

  /**
   * Get all available LLM models
   */
  async getModels(providerId?: string): Promise<LLMModel[]> {
    try {
      console.log('üìã LLM CONFIG: Fetching LLM models (dynamic only)', { providerId });

      // Prefer dynamic, API-key aware models from backend
      const dynamicModels: ModelInfo[] = await dynamicModelsService.getModels({
        providerId,
        useCache: true,
        forceRefresh: false
      });

      const models: LLMModel[] = (dynamicModels || []).map((model) => {
        const costPer1k = model.cost_per_1k_tokens ?? 0.01;
        const context = model.context_length ?? 8192;

        return {
          id: model.id,
          name: model.display_name || model.name,
          providerId: model.provider,
          description: model.display_name || model.name,
          maxTokens: context,
          supportsStreaming: true,
          supportsFunctionCalling: (model.capabilities || []).includes('function-calling'),
          costPerToken: costPer1k / 1000,
          contextWindow: context,
          capabilities: model.capabilities || []
        };
      });

      // Filter by provider if specified
      if (providerId) {
        models = models.filter((model) => model.providerId === providerId);
      }

      console.log(`‚úÖ LLM CONFIG: Loaded ${models.length} models for provider ${providerId || 'ALL'} (dynamic)`);
      return models;
      
    } catch (error) {
      console.error('‚ùå LLM CONFIG: Failed to load models:', error);
      // If dynamic loading fails we return an empty list so the UI
      // can clearly surface that no models are available for this
      // project/API key instead of silently falling back to a tiny
      // hard-coded default list.
      return [];
    }
  }

  /**
   * Get LLM configuration for a specific agent
   */
  async getAgentLLMConfig(projectId: string, agentId: string): Promise<AgentLLMConfig | null> {
    try {
      console.log('üéØ LLM CONFIG: Fetching agent LLM config', { projectId, agentId });
      
      // TODO: Implement API call when backend is ready
      // const response = await fetch(`${this.baseUrl}/projects/${projectId}/agents/${agentId}/llm-config`);
      // if (!response.ok) throw new Error('Failed to fetch agent LLM config');
      // return await response.json();
      
      // For now, return null (will use defaults)
      return null;
      
    } catch (error) {
      console.error('‚ùå LLM CONFIG: Failed to load agent config:', error);
      return null;
    }
  }

  /**
   * Update LLM configuration for a specific agent
   */
  async updateAgentLLMConfig(projectId: string, agentId: string, config: AgentLLMConfig): Promise<AgentLLMConfig> {
    try {
      console.log('üíæ LLM CONFIG: Updating agent LLM config', { projectId, agentId });
      
      // TODO: Implement API call when backend is ready
      // const response = await fetch(`${this.baseUrl}/projects/${projectId}/agents/${agentId}/llm-config`, {
      //   method: 'PUT',
      //   headers: { 'Content-Type': 'application/json' },
      //   body: JSON.stringify(config)
      // });
      // if (!response.ok) throw new Error('Failed to update agent LLM config');
      // const updatedConfig = await response.json();
      
      console.log('‚úÖ LLM CONFIG: Agent LLM config updated successfully');
      return config; // Return the config for now
      
    } catch (error) {
      console.error('‚ùå LLM CONFIG: Failed to update agent config:', error);
      throw error;
    }
  }

  /**
   * Get default LLM configuration for an agent type
   */
  getDefaultConfigForAgentType(agentType: string): Partial<AgentLLMConfig> {
    const defaultConfig = DEFAULT_AGENT_LLM_CONFIGS[agentType];
    
    if (defaultConfig) {
      console.log(`üéØ LLM CONFIG: Using default config for ${agentType}`);
      return defaultConfig;
    }
    
    console.log(`‚ö†Ô∏è LLM CONFIG: No default config for ${agentType}, using AssistantAgent defaults`);
    return DEFAULT_AGENT_LLM_CONFIGS['AssistantAgent'];
  }

  /**
   * Get a complete LLM configuration with defaults filled in
   */
  getCompleteAgentConfig(
    agentId: string,
    agentName: string,
    agentType: string,
    partialConfig?: Partial<AgentLLMConfig>
  ): AgentLLMConfig {
    const defaults = this.getDefaultConfigForAgentType(agentType);
    
    return {
      agentId,
      agentName,
      providerId: partialConfig?.providerId || defaults.providerId || 'openai',
      modelId: partialConfig?.modelId || defaults.modelId || 'gpt-4',
      temperature: partialConfig?.temperature ?? defaults.temperature ?? 0.7,
      maxTokens: partialConfig?.maxTokens || defaults.maxTokens || 2048,
      topP: partialConfig?.topP ?? defaults.topP ?? 1.0,
      frequencyPenalty: partialConfig?.frequencyPenalty ?? defaults.frequencyPenalty ?? 0.0,
      presencePenalty: partialConfig?.presencePenalty ?? defaults.presencePenalty ?? 0.0,
      customConfig: partialConfig?.customConfig || defaults.customConfig || {},
      systemMessage: partialConfig?.systemMessage || defaults.systemMessage
    };
  }

  /**
   * Validate LLM configuration
   */
  validateConfig(config: AgentLLMConfig): { valid: boolean; errors: string[] } {
    const errors: string[] = [];

    if (!config.agentId) {
      errors.push('Agent ID is required');
    }

    if (!config.agentName) {
      errors.push('Agent name is required');
    }

    if (!config.providerId) {
      errors.push('LLM provider is required');
    }

    if (!config.modelId) {
      errors.push('LLM model is required');
    }

    if (config.temperature < 0 || config.temperature > 2) {
      errors.push('Temperature must be between 0 and 2');
    }

    if (config.maxTokens < 1 || config.maxTokens > 32000) {
      errors.push('Max tokens must be between 1 and 32000');
    }

    if (config.topP < 0 || config.topP > 1) {
      errors.push('Top P must be between 0 and 1');
    }

    if (config.frequencyPenalty < -2 || config.frequencyPenalty > 2) {
      errors.push('Frequency penalty must be between -2 and 2');
    }

    if (config.presencePenalty < -2 || config.presencePenalty > 2) {
      errors.push('Presence penalty must be between -2 and 2');
    }

    return {
      valid: errors.length === 0,
      errors
    };
  }

  /**
   * Get LLM provider by ID
   */
  async getProvider(providerId: string): Promise<LLMProvider | null> {
    const providers = await this.getProviders();
    return providers.find(p => p.id === providerId) || null;
  }

  /**
   * Get LLM model by ID
   */
  async getModel(modelId: string): Promise<LLMModel | null> {
    const models = await this.getModels();
    return models.find(m => m.id === modelId) || null;
  }

  /**
   * Get models for a specific provider
   */
  async getModelsForProvider(providerId: string): Promise<LLMModel[]> {
    return this.getModels(providerId);
  }

  /**
   * Calculate estimated cost for a configuration
   */
  async calculateEstimatedCost(config: AgentLLMConfig, estimatedTokens: number): Promise<number> {
    const model = await this.getModel(config.modelId);
    if (!model) return 0;
    
    return model.costPerToken * estimatedTokens;
  }

  /**
   * Get configuration summary for display
   */
  async getConfigSummary(config: AgentLLMConfig): Promise<{
    provider: LLMProvider | null;
    model: LLMModel | null;
    estimatedCostPer1K: number;
  }> {
    const [provider, model] = await Promise.all([
      this.getProvider(config.providerId),
      this.getModel(config.modelId)
    ]);

    const estimatedCostPer1K = model ? model.costPerToken * 1000 : 0;

    return {
      provider,
      model,
      estimatedCostPer1K
    };
  }
}

// Export singleton instance
export const llmConfigService = new LLMConfigurationService();

// Export utility functions
export {
  DEFAULT_LLM_PROVIDERS,
  DEFAULT_LLM_MODELS,
  DEFAULT_AGENT_LLM_CONFIGS
} from '$lib/types/llm-config';
